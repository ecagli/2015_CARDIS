\section{Introduction}
The measurement of the power consumption or of the electromagnetic irradiations during the execution of cryptographic algorithms in constrained electronic devices can reveal information about sensitive variables ({\em e.g.} cryptographic keys). The side channel traces are usually acquired by oscilloscopes with a very high sampling rate, which permits a powerful inspection of the component behaviour, but, at the same time, produces high-dimensional data, that spread the sensitive information over a (sometimes) huge number of time samples. Reducing the dimensionality of the data is an important issue for Side-Channel Attacks (SCA). Considering the side channel traces as column vectors ${\bf x}$ in $\mathbb{R}^\traceLength$, the compressing phase might be seen as the application of a function $\extract\colon \mathbb{R}^\traceLength\rightarrow \mathbb{R}^\newTraceLength$, called {\em extractor} in this paper.\\

The present work focuses on the so-called {\em projecting extractors}, {\em i.e.} those methods that provide extractors $\extract$ whose image components are linear combinations of the original data, or equivalently, expressible {\em via} a matrix multiplication:
\begin{equation}\label{eq:linearExtractor}
\extractor{\sss[]{}} = A\sss[]{} \mbox{ with } A \in M_{\mathbb{R}}(\newTraceLength, \traceLength) \mbox{ ,}
\end{equation}
where $ M_{\mathbb{R}}(\newTraceLength, \traceLength)$ denotes the algebra of real-coefficient matrices of size $\newTraceLength \times \traceLength$.  In particular we effectuate an in-depth study and a comprehensive comparison  between the PCA and the LDA methods \cite{fisher1938statistical,Fukunaga}, and we investigate their exploitability  in Side-Channel context.  Indeed, PCA and LDA are classical statistical procedures, but the way they have been inherited in SCA domain is somehow ambiguous and opened some issues and questions.\\

 The PCA has been applied both in an {\em unsupervised} way, i.e. on the whole data \cite{Batina2012,karsmakers2009side}, and in a {\em supervised} way, i.e. on traces grouped in classes and averaged \cite{TAprincipal,choudaryefficient,choudary2014efficient,disassembler,Standaert2008}. The difference between these two approaches, as also remarked in \cite{disassembler}, implies a huge gap in performances,  and needs to be highlighted. Another ambiguity in PCA concerns the choice of the components that must be kept after the dimension reduction: as already remarked by Specht et al.  \cite{specht}, some papers declare that the leading components are those that contain almost all the useful information \cite{TAprincipal}, while others propose to discard the leading components \cite{Batina2012}. In a specific attack context, Specht et al. compares the results obtained by choosing different subsets of consecutive components, starting from some empirically chosen index. They concluded that for their data the optimal result is obtained by selecting a single component, the fourth one, but they give no formal argumentation about this choice. Such a result is obviously very case-specific. Moreover, the possibility of keeping non-consecutive components is not considered. In this paper we pursue the work in \cite{specht} by proposing a new selection methodology, called {\em Cumulative ELV Selection}, and we argue about its soundness. Our reasonning is essentially based on the assumption that, for secure implementations, the leaking information, if existing, is spread over a few time samples of each trace. This assumption has already been done by Mavroeidis et al. in \cite{SCAclassProbl}, where the authors  also proposed a components selection method. As we will see in this paper, the important difference between their proposal and ours is that we do not discard the information given by the eigenvalues associated to the PCA components.  \\

The introduction of the LDA in SCA literature also revealed some issues: even if declared more meaningful and informative than the PCA method \cite{lessIsMore,Standaert2008}, it is often set aside because of a practical constraint; it is subject to the so-called {\em Small Sample Size problem (SSS)}, i.e. it requires a number of observations (traces) which must be higher than the dimension (size) $\traceLength$ of them. In some contexts it might be an excessive requirement, which may become unacceptable in many practical situations where the amount of observations is very limited. Many propositions to circumvent this problem have been made, especially by Pattern Recognition and Face Recognition communities \cite{eigenfaces,Chen2000,huang,Yu01adirect}. We find mandatory to test and compare such methods in Side-Channel context in order to draw fair conclusions about the practical application of LDA for side-channel analysis.\\

The paper is organised as follows: in Section \ref{sec:preliminaries} we fix notations, recall preliminaries and set up a unified comparison framework to compare different extractors. Section \ref{sec:PCA} presents the PCA, and handles the choice of components problem, introducing the  ELV selection method. In Section \ref{sec:LDA} the LDA method is presented, together with different methodologies to avoid the SSS problem. Experiments and comparisons are showed in Section \ref{sec:experiments}, while conclusions and perspectives follow in Section \ref{sec:conclusions}. 

