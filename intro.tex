\section{Introduction}

In this paper we will focus on the so-called {\em Projecting Extractors}, i.e. those methods that provide extractors $\extract$ whose image components are linear combinations of the original data, or, equivalently, expressible via a matrix multiplication:
\begin{equation}
\extractor{\sss[]{}} = A\sss[]{} \mbox{ with } A \in M_{\mathbb{R}}(\newTraceLength, \traceLength) \mbox{ ,}
\end{equation}
where $ M_{\mathbb{R}}(\newTraceLength, \traceLength)$ denotes the algebra of real-coefficient matrices of size $\newTraceLength \times \traceLength$.  In particular our purpose is to effectuate a comprehensive comparison  between the PCA and the LDA methods, and more precisely, between the different ways to exploit these methods in Side-Channel context. Indeed, PCA and LDA are classical statistical procedures, but the way they are have been inherited in Side-Channel Attacks (SCA for short) domain is somehow ambiguous and opened some issues and questions.\\
PCA has been applied both in an {\em unsupervised} way, i.e. on the whole data \cite{Batina2012,karsmakers2009side}, and in a {\em supervised} way, i.e. on traces grouped in classes and averaged \cite{TAprincipal,choudaryefficient,choudary2014efficient,disassembler,Standaert2008}. Another ambiguity in PCA concerns the choice of the components that must be kept after the dimension reduction: as also remarked by Specht et al. \cite{}, some papers declare that the leading components are those that contain almost all the useful information \cite{TAprincipal}, while others propose to discard the leading components \cite{Batina2012}. Specht \textit{et al.} compared, in a specific attack context, the results obtained by choosing different subsets of consecutive components, starting from some empirically chosen index, and concluded that for their data the optimal result is obtained by selecting a single component, the fourth one (with no real argumentation about this choice). Such a result is obviously very case-specific. Moreover, the possibility of keeping non-consecutive components is not considered. In this paper we propose a new selection methodology, called {\em Cumulative ELV Selection}. It is based on assumptions close to those proposed by Mavroeidis \textit{et al.} \cite{SCAclassProbl} but with the important difference that we do not discard the information given by the eigenvalues associated to the PCA components. Following the introduction of our new method, we study its efficiency/effectiveness and we compare it to other PoI methods through a unified comparison framework. \\

The introduction of the LDA in SCA literature also made some issues arise: even if declared more meaningful and informative than the PCA method \cite{Standaert2008}, it is often set aside because of a practical constraint; it is subject to the so-called {\em Small Sample Size problem (SSS)}, i.e. it requires a number of observations (traces) which is higher than the dimension (size) $\traceLength$ of them. In some contexts it might be an excessive requirement, which may become unacceptable in many practical contexts where the amount of observations is very limited. Nevertheless, many propositions to circumvent this problem have been made, especially by Pattern Recognition and Face Recognition communities \cite{eigenfaces,Chen2000,huang,Yu01adirect}. We find mandatory to test and compare such methods in Side-Channel context.