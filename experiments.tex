\section{Experimental Results}\label{sec:experiments}

In this section we compare the different extractors provided by the PCA and the LDA in association with the different techniques  of components selection. Defining an universal criterion to compare the different extractors would not make sense since the latter one should encompass a lot of parameters, sometimes opposite, that vary according to the context (amount of noise, specificity of the information leakage, nature of the side channel, etc.). For this reason we will choose to split our comparisons according to four contexts depending on the final goal pursued by the attacker: 
\begin{enumerate}
\item {\em Minimize $\numTraces[]$}: achieve $\guessingEntropy_{\adversary'}\leq \threshold$ with the minimal number of attack traces, with $\threshold$ a fixed threshold, common to the four goals
\item {\em Minimize $\numTraces[]'$}: achieve $\guessingEntropy_{\adversary'}\leq \threshold$ with the minimal number of profiling traces
\item {\em Minimize $\newTraceLength$}: achieve $\guessingEntropy_{\adversary'}\leq \threshold$ reducing as much as possible the size of the extracted traces
\item {\em Minimize $\numPoI$}: achieve $\guessingEntropy_{\adversary'}\leq \threshold$ exploiting the minimal number of original trace points.
\end{enumerate}
 
 An attack scenario has been defined for each of the goals above: in each one, three of the four parameters $\numTraces[], \numTraces[]', \newTraceLength, \numPoI$ are fixed and one varies. For those in which $\numTraces[]'$ is fixed, the value of $\numTraces[]'$ is chosen high enough to avoid the SSS problem, and the extensions of LDA presented in Sec.~\ref{sec:SSS} are not evaluated.
 \footnote{This study is let open for an extended version of this paper.} 
 This choice of $\numTraces[]'$ will imply that the LDA is always performed in a favourable situation, which makes expect the LDA to be particularly efficient for these experiments. Consequentely, for the scenarios in which $\numTraces[]'$ is high, our goal is to study whether the PCA can be made almost as efficient as the LDA thanks to the component selection methods discussed in Sec.~\ref{sec:ELV}. 
%The extensions of the LDA presented in Sec.~\ref{sec:SSS} are evaluated through the Scenario 2, where $\numTraces[]'$ varies.



\subsubsection{The testing adversary.}  Our testing adversary attacks an 8-bit AVR microprocessor Atmega328P and acquires power-consumption traces via the ChipWhisperer platform \cite{o2014chipwhisperer}.\footnote{This choice has been done to allow for reproducibility of the experiments.} The target device stores a secret 128-bit key and performs the first steps of an AES: the loading of 16 bytes of the plaintext, the AddRoundKey step and the AES Sbox. It has been programmed twice: two different keys are stored in the device memory during the acquisition of the profiling and of the attack traces, to simulate the situation of two identical devices storing a different secret. The size $\traceLength$ of the traces equals $3996$. The sensitive target  variable is the output of the first Sbox processing, but, since the key is fixed also during the profiling phase, and both Xor and Sbox operations are bijective, we expect to detect three interesting regions (as those high-lighted by PCs 4, 5 and 6, in Fig.~\ref{fig:6components}): the reading of the first byte of the plaintext, the first AddRoundKey and the first Sbox. We consider an {\em identity classification} leaking function (i.e. we make minimal assumption on the leakage function), which implies that the 256 possible values of the Sbox output yields to 256 classes. For each class we assume that the adversary acquires the same number $N_p$ of traces, \textit{i.e.} $\numTraces[]' = N_p\times 256$. After the application of the extractor $\extract$, the trace size is reduced to $\newTraceLength$. Then the attacker performs a Bayesian Template Attack \cite{Chari2003}, using $\newTraceLength$-variate Gaussian templates. This choice comes from the information-theoretic optimality of such an attack which, exploiting the maximum likelihood parameters estimation, yields to an unbiased comparison between the extractors.


\begin{figure}[t]
\subfigure[]{\label{fig:1.1}
\includegraphics[width=0.5\textwidth]{figures/Criterion1.pdf}}
\subfigure[]{\label{fig:1.2}
\includegraphics[width=0.5\textwidth]{figures/Criterion1Good.pdf}}
\caption{Guessing Entropy as function of the number of attack traces for different extraction methods. All Guessing Entropies are estimated as the average rank of the right key over 100 independent experiments.}\label{fig:scenario1}
\end{figure}
\subsubsection{Scenario 1.}
To analyse the dependence between the extraction methods presented in Sections~\ref{sec:PCA} and \ref{sec:LDA} and the number of attack traces $\numTraces[]$ needed to achieve a given GE, we fixed the other parameters as follows: $N_p=50$ ($\numTraces[]'=50\times 256$), $\newTraceLength = 3$ and $\numPoI = 3996$ (all points are allowed to participate in the building of the PCs and of the LDCs). The experimental results, depicted in Fig.~\ref{fig:scenario1}\subref{fig:1.1}-\subref{fig:1.2}, show that the PCA standard method has very bad performances in SCA, while the LDA outperforms the others. Concerning the class-oriented PCA, we observe that its performance is close to that of LDA when combined with the selection methods ELV (which performs best) or IPR.  



\subsubsection{Scenario 2.}
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS.pdf}
%\includegraphics[width=0.5\textwidth]{figures/Criterion2notSSS.pdf} 
%\caption{Guessing Entropy as function of the number of profiling traces per class, for different extraction methods: on the left the LDA is substituted by its extensions to handle the SSS problem.}\label{fig:2}
%\end{figure}
Now we test the behaviour of the extraction methods as function of the number $N_p$ of available profiling traces per class. The number of components $\newTraceLength$ is still fixed to 3, $\numPoI=3996$ again and the number of attack traces is $\numTraces[]=100$. This scenario has to be divided into two parts: if $N_p\leq 15$, then $\numTraces[]'<\traceLength$ and the SSS problem occurs. Thus, in this case we test the four extensions of LDA presented in Sec.~\ref{sec:SSS}, associated to either the standard selection, to which we abusively refer as EGV,%\footnote{It consists in keeping the $\newTraceLength$ first LDCs, except for the Direct LDA, which asks to keep the last LDCs.}
\footnote{It consists in keeping the $\newTraceLength$ first LDCs (the $C$ last for the Direct LDA)}
or to the IPR selection.  We compare them to the class-oriented PCA associated to EGV, IPR or ELV. The ELV selection is not performed for the techniques extending LDA, since for some of them the projecting LDCs are not associated to some eigenvalues in a meaningful way. On the contrary, if $N_p\geq 16$ there is no need to approximate the LDA technique, so the classical one is performed. Results for this scenario are shown in Fig.~\ref{fig:scenario2}. It may be noticed that the combinations class-oriented PCA + ELV/IPR select exactly the same components, for our data, see Fig.~\ref{fig:pcaclass} and do not suffer from the lack of profiling traces. They are slightly outperformed by the $\SW$ Null Space method associated with the EGV, see Fig.\ref{fig:swnullspace}. The Direct LDA (Fig.~\ref{fig:direct}) method also provides a good alternative, while the other tested methods do not show a stable behaviour. The results in absence of the SSS problem (Fig.\ref{fig:notSSS}) confirm that the standard PCA is not adapted to SCA, even when provided with more profiling traces. It also shows that among class-oriented PCA and LDA, the class-oriented PCA converges faster.



\begin{figure}
\subfigure[]{\label{fig:fisherface}
\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS_Fisherface.pdf}}
\subfigure[]{\label{fig:direct}
\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS_DirectLDA.pdf}}
\subfigure[]{\label{fig:stspannedspace}
\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS_STSpannedSpace.pdf}}
\subfigure[]{\label{fig:swnullspace}
\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS_SWnullSpace.pdf}}
\subfigure[]{\label{fig:pcaclass}
\includegraphics[width=0.5\textwidth]{figures/Criterion2SSS_PCAclass.pdf}}
\subfigure[]{\label{fig:notSSS}
\includegraphics[width=0.5\textwidth]{figures/Criterion2notSSS.pdf}}
\caption{Guessing Entropy as function of the number of profiling traces. Figures \subref{fig:fisherface}-\subref{fig:swnullspace}: methods extending the LDA in presence of SSS problem; Figure \subref{fig:pcaclass}: class-oriented PCA in presence of the SSS problem; Figure \subref{fig:notSSS}: number of profiling traces high enough to avoid the SSS problem.}\label{fig:scenario2}
\end{figure}

\subsubsection{Scenario 3.}
Let  $\newTraceLength$ be now variable and let the other parameters be fixed as follows: $\numTraces[] = 100, N_p=200, \numPoI = 3996$. Looking at Fig.~\ref{fig:3}, we might observe that the standard PCA might actually well perform in SCA context if provided with a larger number of kept components; on the contrary, a little number of components suffices to the LDA. Finally, keeping more of the necessary components does not worsen the efficiency of the attack, which allows the attacker to choose $\newTraceLength$ as the maximum value supported by his computational means.

\begin{remark}
In our experiments the ELV selection method only slightly outperforms the IPR. Nevertheless, since it relies on more sound and more general observations, {\em i.e.} the maximization of explained variance concentrated into few points, it is likely to be more robust and less case-specific. For example, in Fig.~\ref{fig:notSSS} it can be remarked that while the class-oriented PCA + ELV line keeps constant on the value 0 of guessing entropy, the class-oriented PCA + IPR is sometimes higher than 0.
\end{remark}

\medskip

\medskip

  \begin{minipage}[c]{\textwidth}
  \hspace*{-3mm}
  \begin{minipage}[c]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/Criterion3.pdf}
    \captionof{figure}{Guessing Entropy as function of the number of the traces size after reduction}\label{fig:3}
  \end{minipage}
\hspace{1mm}
  \begin{minipage}[c]{0.45\textwidth}
    \centering
    \begin{tiny}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&&\multicolumn{4}{|>{\columncolor[gray]{0.7}}c|}{Parameter to minimize}\\
\hline
\multicolumn{1}{|>{\columncolor[gray]{0.7}}c|}{Method}&\multicolumn{1}{|>{\columncolor[gray]{0.7}}c|}{Selection}& $N$ &  $N'$ (SSS) &  $N'$ ($\neg$SSS) &  $C$ \\
\hline
PCA standard & EGV & {\bf -} &  &{\bf -} &{\bf -} \\
\hline
PCA standard &\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{ELV} & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} \\
\hline
PCA standard & IPR &{\bf -} & &{\bf -} &{\bf +} \\
\hline
PCA class & EGV & {\bf -} &{\bf -} &{\bf -} &{\bf -} \\
\hline
PCA class & \multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{ELV} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$}&\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$} &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} \\
\hline 
PCA class & IPR & {\bf {\bf +}} &$\bigstar$ &{\bf +} &{\bf -} \\
\hline 
LDA & EGV &$\bigstar$ & & {\bf +} & $\bigstar$\\
\hline 
LDA & \multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{ELV} & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} &  & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$}\\
\hline 
LDA & IPR & {\bf +} & &{\bf +} & $\bigstar$ \\

\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{$\SW$ Null Space}  & EGV & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$ } & & \\
\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{$\SW$ Null Space}  & IPR & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}} & & \\
\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{Direct LDA} & EGV & & \multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{$\bigstar$}& & \\
\hline 
\multicolumn{1}{|>{\columncolor[gray]{0.8}}c|}{Direct LDA} & IPR & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf +}}& & \\
\hline
\multicolumn{2}{|>{\columncolor[gray]{0.8}}c|}{Fisherface} & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} & & \\
\hline 
\multicolumn{2}{|>{\columncolor[gray]{0.8}}c|}{$\ST$ Spanned Space}  & &\multicolumn{1}{|>{\columncolor[gray]{0.9}}c|}{{\bf -}} & & \\
\hline
\end{tabular}
\end{tiny}
\captionof{table}{Overview of extractors performances in tested situations.}\label{table:results}
    \end{minipage}
  \end{minipage}

\begin{figure}
\includegraphics[width=0.5\textwidth]{figures/Criterion4.pdf}
\includegraphics[width=0.5\textwidth]{figures/Criterion4cutted.pdf} 
\caption{Guessing Entropy as function of the number of time samples contributing to the extractor computation.}\label{fig:4}
\end{figure}

An overview of the results of our comparison in scenarios 1, 2 and 3 is depicted in Table~\ref{table:results}: depending on the adversary purpose, given by the parameter to minimize, a $\bigstar$ denotes the best method, a ${\bf +}$ denotes a method with performances close to those of the best one and a ${\bf -}$ is for methods that show lower performances. Techniques introduced in this paper are highlighted by a grey background.  For example we remark that the class-oriented PCA takes advantage of the association with our ELV selection of components, achieving optimal performances when the goal is to minimize the number of profiling traces $\numTraces[]'$. As expected, when there are no constraints over $\numTraces[]'$, the LDA outperforms the other methods; however, even in this case which is very favourable to the LDA, the class-oriented PCA equipped with the ELV selection has an efficiency which is close to that of the LDA.



\subsubsection{Scenario 4.}


This is the single scenario in which we allow the ELV selection method to not only select the components to keep but also to modify them, keeping only some coefficients within each component, setting the other ones to zero. We select pairs \textit{(component, time sample)} in decreasing order of the ELV values, allowing the presence of only $\newTraceLength = 3$ components and $\numPoI$ different times samples: {\em i.e.}, we impose that the matrix $A$ defining the extractor (see \eqref{eq:linearExtractor}) has $\newTraceLength = 3$ rows (storing the 3 chosen components, transposed) and exactly $\numPoI$ non-zero columns.
Looking at Fig.~\ref{fig:4} we might observe that the LDA allows to achieve the maximal guessing entropy with only 1 PoI in each of the 3 selected components. 
Actually, adding PoIs worsen its performances, which is coherent with the assumption that the vulnerable information leaks in only a few points. Such points are excellently detected by the LDA. Adding contribution from other points raises the noise, which is then compensated by the contributions of further noisy points, in a very delicate balance. Such a behaviour is clearly visible in standard PCA case: the first 10 points considered raise the level of noise, that is then balanced by the last 1000 points.









